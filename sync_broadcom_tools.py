import os
import requests
import time
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import hashlib
import logging
import sys
import datetime
import concurrent.futures
import re
from pathlib import Path
import argparse
import platform

# é…ç½®
BASE_URL = "https://packages-prod.broadcom.com/tools/"
DEFAULT_LOCAL_ROOT = os.path.join("VMware Tools", "tools")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.5"
}
MAX_RETRIES = 3
RETRY_DELAY = 5  # ç§’
DEFAULT_LOG_FILE = "vmware_tools_sync.log"

# è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
def setup_logger(log_file):
    logger = logging.getLogger('VMwareToolsSync')
    logger.setLevel(logging.INFO)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def get_remote_file_info(url, session):
    """è·å–è¿œç¨‹æ–‡ä»¶ä¿¡æ¯"""
    file_info = {'size': 0, 'last_modified': None, 'etag': None}
    try:
        response = session.head(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        # è·å–æ–‡ä»¶å¤§å°
        file_info['size'] = int(response.headers.get("Content-Length", 0))
        
        # è·å–æœ€åä¿®æ”¹æ—¶é—´
        last_modified = response.headers.get("Last-Modified")
        if last_modified:
            try:
                file_info['last_modified'] = time.mktime(
                    datetime.datetime.strptime(last_modified, "%a, %d %b %Y %H:%M:%S %Z").timetuple())
            except Exception:
                pass
        
        # è·å–ETag
        file_info['etag'] = response.headers.get("ETag", "").strip('"')
    
    except requests.RequestException as e:
        logger.warning(f"âš ï¸ æ— æ³•è·å–è¿œç¨‹æ–‡ä»¶ä¿¡æ¯: {url} - {e}")
    
    return file_info

def should_download(url, local_path, remote_info, session):
    """æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½æ–‡ä»¶"""
    # æ–°æ–‡ä»¶
    if not os.path.exists(local_path):
        logger.info(f"  æ–°æ–‡ä»¶")
        return True
    
    try:
        # è·å–æœ¬åœ°æ–‡ä»¶ä¿¡æ¯
        local_size = os.path.getsize(local_path)
        local_mtime = os.path.getmtime(local_path)
        
        logger.info(f"ğŸ” æ£€æŸ¥æ–‡ä»¶: {os.path.basename(local_path)}")
        logger.info(f"  æœ¬åœ°å¤§å°: {local_size} | è¿œç¨‹å¤§å°: {remote_info['size']}")
        
        # å¤§å°æ£€æŸ¥
        if local_size != remote_info['size'] and remote_info['size'] > 0:
            logger.info(f"  æ–‡ä»¶å¤§å°å˜åŒ–: {local_size} â†’ {remote_info['size']}")
            return True
        
        # ä¿®æ”¹æ—¶é—´æ£€æŸ¥
        if remote_info['last_modified'] and remote_info['last_modified'] > local_mtime:
            logger.info(f"  è¿œç¨‹æ–‡ä»¶æ›´æ–°: {time.ctime(local_mtime)} â†’ {time.ctime(remote_info['last_modified'])}")
            return True
        
        # å“ˆå¸Œæ£€æŸ¥ï¼ˆå¦‚æœETagå¯ç”¨ï¼‰
        if remote_info['etag']:
            local_hash = calculate_file_hash(local_path)
            if local_hash and remote_info['etag'] != local_hash:
                logger.info(f"  å“ˆå¸Œå€¼ä¸åŒ¹é…: æœ¬åœ°={local_hash[:8]}... è¿œç¨‹={remote_info['etag'][:8]}...")
                return True
        
        logger.info(f"  æ–‡ä»¶æœªå˜æ›´")
        return False
    
    except Exception as e:
        logger.warning(f"âš ï¸ æ–‡ä»¶æ£€æŸ¥å‡ºé”™: {url} - {e}")
        return True  # å‡ºé”™æ—¶ä¸‹è½½ä»¥ç¡®ä¿æ›´æ–°

def calculate_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶çš„SHA-256å“ˆå¸Œå€¼"""
    sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            while chunk := f.read(8192):
                sha256.update(chunk)
        return sha256.hexdigest()
    except Exception as e:
        logger.error(f"âŒ è®¡ç®—å“ˆå¸Œå€¼æ—¶å‡ºé”™ {file_path}: {e}")
        return None

def download_file(task):
    """ä¸‹è½½å•ä¸ªæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
    url, local_path, remote_info, session = task
    temp_path = f"{local_path}.tmp"
    
    try:
        logger.info(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {url}")
        start_time = time.time()
        
        with session.get(url, headers=HEADERS, stream=True, timeout=60) as r:
            r.raise_for_status()
            total_size = int(r.headers.get("Content-Length", remote_info['size']))
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            with open(temp_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # è·å–æœ€åä¿®æ”¹æ—¶é—´
            last_modified_header = r.headers.get('Last-Modified')
        
        # éªŒè¯ä¸‹è½½
        downloaded_size = os.path.getsize(temp_path)
        if total_size > 0 and downloaded_size != total_size:
            raise IOError(f"å¤§å°ä¸åŒ¹é…: é¢„æœŸ {total_size}, å®é™… {downloaded_size}")
        
        # æ›¿æ¢æ—§æ–‡ä»¶
        if os.path.exists(local_path):
            os.remove(local_path)
        os.rename(temp_path, local_path)
        
        # è®¾ç½®æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        if last_modified_header:
            try:
                remote_time = time.mktime(
                    datetime.datetime.strptime(last_modified_header, "%a, %d %b %Y %H:%M:%S %Z").timetuple())
                os.utime(local_path, (remote_time, remote_time))
                logger.debug(f"  æ–‡ä»¶ä¿®æ”¹æ—¶é—´å·²è®¾ç½®ä¸º: {last_modified_header}")
            except Exception as e:
                logger.warning(f"âš ï¸ è®¾ç½®ä¿®æ”¹æ—¶é—´å¤±è´¥: {e}")
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        dl_time = time.time() - start_time
        size_mb = downloaded_size / (1024 * 1024)
        speed = size_mb / dl_time if dl_time > 0 else 0
        logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {url}")
        logger.info(f"  å¤§å°: {size_mb:.2f} MB | ç”¨æ—¶: {dl_time:.2f}ç§’ | é€Ÿåº¦: {speed:.2f} MB/s")
        return True
    
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {url}: {str(e)[:200]}")
        if os.path.exists(temp_path):
            try:
                os.remove(temp_path)
            except:
                pass
        return False

def process_directory(url, local_dir, session):
    """å¤„ç†ç›®å½•åŠå…¶å†…å®¹ï¼ˆè¿”å›éœ€è¦ä¸‹è½½çš„æ–‡ä»¶åˆ—è¡¨ï¼‰"""
    logger.info(f"\nğŸ“‚ å¤„ç†ç›®å½•: {url}")
    download_tasks = []
    
    try:
        response = session.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for link in soup.find_all('a'):
            href = link.get('href')
            if not href or href in ["../", "./"] or href.startswith(("?", "#", "javascript:")):
                continue
            
            full_url = urljoin(url, href)
            item_name = href.rstrip('/')
            
            # å¤„ç†ç›®å½•
            if href.endswith('/'):
                sub_dir = os.path.join(local_dir, item_name)
                os.makedirs(sub_dir, exist_ok=True)
                logger.info(f"  â”œâ”€ è¿›å…¥å­ç›®å½•: {item_name}/")
                download_tasks.extend(process_directory(full_url, sub_dir, session))
            # å¤„ç†æ–‡ä»¶
            else:
                local_path = os.path.join(local_dir, item_name)
                remote_info = get_remote_file_info(full_url, session)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
                if should_download(full_url, local_path, remote_info, session):
                    download_tasks.append((full_url, local_path, remote_info, session))
    
    except Exception as e:
        logger.error(f"âŒ å¤„ç†ç›®å½•å‡ºé”™ {url}: {e}")
    
    return download_tasks

def get_cpu_count():
    """è·å–CPUæ ¸å¿ƒæ•°ï¼Œç”¨äºç¡®å®šçº¿ç¨‹æ•°"""
    try:
        return os.cpu_count() or 4
    except:
        return 4

def main():
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='VMware Tools åŒæ­¥å·¥å…·')
    parser.add_argument('--local-dir', type=str, default=DEFAULT_LOCAL_ROOT, 
                        help=f'æœ¬åœ°å­˜å‚¨ç›®å½• (é»˜è®¤: {DEFAULT_LOCAL_ROOT})')
    parser.add_argument('--log-file', type=str, default=DEFAULT_LOG_FILE, 
                        help=f'æ—¥å¿—æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {DEFAULT_LOG_FILE})')
    parser.add_argument('--threads', type=int, default=0, 
                        help='çº¿ç¨‹æ•° (é»˜è®¤: æ ¹æ®CPUæ ¸å¿ƒæ•°è‡ªåŠ¨è®¾ç½®)')
    parser.add_argument('--retries', type=int, default=MAX_RETRIES, 
                        help=f'é‡è¯•æ¬¡æ•° (é»˜è®¤: {MAX_RETRIES})')
    parser.add_argument('--delay', type=int, default=RETRY_DELAY, 
                        help=f'é‡è¯•å»¶è¿Ÿ (ç§’) (é»˜è®¤: {RETRY_DELAY})')
    parser.add_argument('--full-sync', action='store_true', 
                        help='å¼ºåˆ¶å®Œå…¨åŒæ­¥ (å¿½ç•¥æœ¬åœ°æ–‡ä»¶)')
    
    args = parser.parse_args()
    
    # è®¾ç½®å…¨å±€logger
    global logger
    logger = setup_logger(args.log_file)
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    logger.info(f"\n{'=' * 80}")
    logger.info(f"ğŸš€ å¼€å§‹ VMware Tools åŒæ­¥")
    logger.info(f"ğŸ–¥ï¸ ç³»ç»Ÿ: {platform.system()} {platform.release()} ({platform.machine()})")
    logger.info(f"ğŸ’» CPU: {os.cpu_count()} æ ¸å¿ƒ")
    logger.info(f"ğŸ“ æœ¬åœ°ç›®å½•: {os.path.abspath(args.local_dir)}")
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {os.path.abspath(args.log_file)}")
    logger.info(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"{'=' * 80}\n")
    
    start_time = time.time()
    
    # åˆ›å»ºä¼šè¯å¯¹è±¡
    session = requests.Session()
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦ä¸‹è½½çš„æ–‡ä»¶
    all_tasks = process_directory(BASE_URL, args.local_dir, session)
    
    if args.full_sync:
        logger.info("ğŸ” å¼ºåˆ¶å®Œå…¨åŒæ­¥æ¨¡å¼ - æ‰€æœ‰æ–‡ä»¶å°†è¢«ä¸‹è½½")
    
    # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
    download_tasks = []
    for task in all_tasks:
        url, local_path, remote_info, session_ref = task
        if args.full_sync or should_download(url, local_path, remote_info, session):
            download_tasks.append((url, local_path, remote_info, session))
    
    logger.info(f"ğŸ“‹ å‘ç° {len(download_tasks)} ä¸ªæ–‡ä»¶éœ€è¦ä¸‹è½½")
    
    # è®¾ç½®çº¿ç¨‹æ•°
    thread_count = args.threads or min(get_cpu_count() * 2, 16)  # æœ€å¤š16çº¿ç¨‹
    logger.info(f"ğŸ§µ ä½¿ç”¨ {thread_count} çº¿ç¨‹è¿›è¡Œä¸‹è½½")
    
    # å¤šçº¿ç¨‹ä¸‹è½½
    completed = 0
    failed = 0
    with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {executor.submit(download_file, task): task for task in download_tasks}
        
        # å¤„ç†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_task):
            task = future_to_task[future]
            try:
                result = future.result()
                if result:
                    completed += 1
                else:
                    failed += 1
            except Exception as e:
                logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå‡ºé”™: {e}")
                failed += 1
    
    # ç»Ÿè®¡ç»“æœ
    duration = time.time() - start_time
    logger.info(f"\n{'=' * 80}")
    logger.info(f"âœ… åŒæ­¥å®Œæˆ! ç”¨æ—¶: {duration:.2f} ç§’")
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  æ€»æ–‡ä»¶æ•°: {len(all_tasks)}")
    logger.info(f"  éœ€è¦ä¸‹è½½: {len(download_tasks)}")
    logger.info(f"  æˆåŠŸä¸‹è½½: {completed}")
    logger.info(f"  ä¸‹è½½å¤±è´¥: {failed}")
    logger.info(f"â° ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"{'=' * 80}")
    
    # æ·»åŠ ç©ºè¡Œåˆ†éš”æ¯æ¬¡è¿è¡Œ
    with open(args.log_file, "a", encoding="utf-8") as log_file:
        log_file.write("\n\n")

if __name__ == "__main__":
    main()
