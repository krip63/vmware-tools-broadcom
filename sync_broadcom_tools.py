import os
import requests
import time
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import hashlib
import logging
import sys
import datetime

# Configuration
BASE_URL = "https://packages-prod.broadcom.com/tools/"
LOCAL_ROOT = os.path.join("VMware Tools", "tools")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.5"
}
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds
LOG_FILE = "sync_log.txt"

# è®¾ç½®æ—¥å¿—ç³»ç»Ÿ - è¯¦ç»†è®°å½•åˆ°æ–‡ä»¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

def log(message, level=logging.INFO):
    """ç»Ÿä¸€æ—¥å¿—è®°å½•å‡½æ•°"""
    logger.log(level, message)
    # ç¡®ä¿ç«‹å³åˆ·æ–°åˆ°æ–‡ä»¶
    logging.getLogger().handlers[0].flush()

def fetch_url(url):
    """è·å–URLå†…å®¹ï¼Œæ”¯æŒé‡è¯•"""
    for attempt in range(MAX_RETRIES):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            if attempt < MAX_RETRIES - 1:
                log(f"é‡è¯• {attempt+1}/{MAX_RETRIES} - {url}", logging.WARNING)
                time.sleep(RETRY_DELAY * (attempt + 1))
            else:
                log(f"âŒ æ— æ³•è·å– {url}: {e}", logging.ERROR)
                return None

def calculate_file_hash(file_path):
    """è®¡ç®—æ–‡ä»¶çš„SHA-256å“ˆå¸Œå€¼"""
    sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            while chunk := f.read(8192):
                sha256.update(chunk)
        return sha256.hexdigest()
    except Exception as e:
        log(f"âŒ è®¡ç®—å“ˆå¸Œå€¼æ—¶å‡ºé”™ {file_path}: {e}", logging.ERROR)
        return None

def download_file(url, local_path):
    """ä¸‹è½½æ–‡ä»¶ï¼ˆä»…å½“éœ€è¦æ›´æ–°æ—¶ï¼‰"""
    # åˆ›å»ºç›®å½•ç»“æ„
    os.makedirs(os.path.dirname(local_path), exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
    file_changed = False
    skip_reason = "æœªæ£€æŸ¥"
    
    if os.path.exists(local_path):
        try:
            # è·å–æœ¬åœ°æ–‡ä»¶ä¿¡æ¯
            local_size = os.path.getsize(local_path)
            local_mtime = os.path.getmtime(local_path)
            local_hash = calculate_file_hash(local_path)
            
            # è·å–è¿œç¨‹æ–‡ä»¶ä¿¡æ¯
            response = requests.head(url, headers=HEADERS, timeout=15)
            response.raise_for_status()
            
            remote_size = int(response.headers.get("Content-Length", 0))
            last_modified = response.headers.get("Last-Modified")
            etag = response.headers.get("ETag", "").strip('"')
            
            # è®°å½•æ–‡ä»¶æ¯”è¾ƒä¿¡æ¯
            log(f"ğŸ” æ£€æŸ¥æ–‡ä»¶: {os.path.basename(local_path)}")
            log(f"  æœ¬åœ°å¤§å°: {local_size} | è¿œç¨‹å¤§å°: {remote_size}")
            
            if local_size != remote_size:
                file_changed = True
                log(f"  æ–‡ä»¶å¤§å°å˜åŒ–: {local_size} â†’ {remote_size}")
            elif etag and local_hash and etag != local_hash:
                file_changed = True
                log(f"  å“ˆå¸Œå€¼ä¸åŒ¹é…: æœ¬åœ°={local_hash[:8]}... è¿œç¨‹={etag[:8]}...")
            elif last_modified:
                # æ¯”è¾ƒä¿®æ”¹æ—¶é—´
                remote_time = time.mktime(datetime.datetime.strptime(
                    last_modified, "%a, %d %b %Y %H:%M:%S %Z").timetuple())
                if remote_time > local_mtime:
                    file_changed = True
                    log(f"  è¿œç¨‹æ–‡ä»¶æ›´æ–°: {time.ctime(local_mtime)} â†’ {time.ctime(remote_time)}")
            else:
                skip_reason = "æ–‡ä»¶æœªå˜æ›´"
                log(f"  æ–‡ä»¶æœªå˜æ›´")
        except Exception as e:
            log(f"âš ï¸ æ— æ³•éªŒè¯è¿œç¨‹æ–‡ä»¶: {url} - {e}", logging.WARNING)
            file_changed = True  # å‡ºé”™æ—¶ä¸‹è½½ä»¥ç¡®ä¿æ›´æ–°
    else:
        file_changed = True
        log(f"  æ–°æ–‡ä»¶")
    
    # å¦‚æœä¸éœ€è¦ä¸‹è½½
    if not file_changed:
        log(f"â­ï¸ è·³è¿‡: {url} ({skip_reason})")
        return False
    
    # æ‰§è¡Œä¸‹è½½
    temp_path = f"{local_path}.tmp"
    start_time = time.time()
    
    for attempt in range(MAX_RETRIES):
        try:
            log(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {url}")
            with requests.get(url, headers=HEADERS, stream=True, timeout=60) as r:
                r.raise_for_status()
                total_size = int(r.headers.get("Content-Length", 0))
                downloaded = 0
                
                with open(temp_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            # è¿›åº¦æ—¥å¿—
                            if total_size > 0 and attempt == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶è®°å½•è¿›åº¦
                                percent = (downloaded / total_size) * 100
                                if percent % 10 == 0:  # æ¯10%è®°å½•ä¸€æ¬¡è¿›åº¦
                                    log(f"  è¿›åº¦: {percent:.0f}% ({downloaded}/{total_size} å­—èŠ‚)")
            
            # éªŒè¯ä¸‹è½½
            downloaded_size = os.path.getsize(temp_path)
            if total_size > 0 and downloaded_size != total_size:
                raise IOError(f"å¤§å°ä¸åŒ¹é…: é¢„æœŸ {total_size}, å®é™… {downloaded_size}")
            
            # è®¾ç½®æ–‡ä»¶ä¿®æ”¹æ—¶é—´ä¸ºè¿œç¨‹æ—¶é—´
            if last_modified:
                remote_time = time.mktime(datetime.datetime.strptime(
                    last_modified, "%a, %d %b %Y %H:%M:%S %Z").timetuple())
                os.utime(temp_path, (remote_time, remote_time))
            
            # æ›¿æ¢æ—§æ–‡ä»¶
            if os.path.exists(local_path):
                os.remove(local_path)
            os.rename(temp_path, local_path)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            dl_time = time.time() - start_time
            size_mb = downloaded_size / (1024 * 1024)
            speed = size_mb / dl_time if dl_time > 0 else 0
            log(f"âœ… ä¸‹è½½å®Œæˆ: {url}")
            log(f"  å¤§å°: {size_mb:.2f} MB | ç”¨æ—¶: {dl_time:.2f}ç§’ | é€Ÿåº¦: {speed:.2f} MB/s")
            return True
        
        except Exception as e:
            if attempt < MAX_RETRIES - 1:
                log(f"ğŸ”„ é‡è¯• {attempt+1}/{MAX_RETRIES} - {url}: {e}", logging.WARNING)
                time.sleep(RETRY_DELAY * (attempt + 1))
            else:
                log(f"âŒ ä¸‹è½½å¤±è´¥: {url}: {e}", logging.ERROR)
            if os.path.exists(temp_path):
                try:
                    os.remove(temp_path)
                except:
                    pass
    
    return False

def process_directory(url, local_dir):
    """å¤„ç†ç›®å½•åŠå…¶å†…å®¹"""
    log(f"\nğŸ“‚ å¤„ç†ç›®å½•: {url}")
    response = fetch_url(url)
    if not response:
        return 0, 0
    
    try:
        soup = BeautifulSoup(response.text, 'html.parser')
        processed_items = 0
        skipped_items = 0
        
        for link in soup.find_all('a'):
            href = link.get('href')
            if not href or href in ["../", "./"] or href.startswith(("?", "#", "javascript:")):
                continue
            
            full_url = urljoin(url, href)
            item_name = href.rstrip('/')
            
            # å¤„ç†ç›®å½•
            if href.endswith('/'):
                sub_dir = os.path.join(local_dir, item_name)
                os.makedirs(sub_dir, exist_ok=True)
                log(f"  â”œâ”€ è¿›å…¥å­ç›®å½•: {item_name}/")
                sub_processed, sub_skipped = process_directory(full_url, sub_dir)
                processed_items += sub_processed
                skipped_items += sub_skipped
            # å¤„ç†æ–‡ä»¶
            else:
                local_path = os.path.join(local_dir, item_name)
                if download_file(full_url, local_path):
                    processed_items += 1
                else:
                    skipped_items += 1
        
        log(f"ğŸ“Š ç›®å½•ç»Ÿè®¡: {url}")
        log(f"  å·²å¤„ç†: {processed_items} | å·²è·³è¿‡: {skipped_items}")
        return processed_items, skipped_items
    
    except Exception as e:
        log(f"âŒ å¤„ç†ç›®å½•å‡ºé”™ {url}: {e}", logging.ERROR)
        return 0, 0

if __name__ == "__main__":
    log(f"\n{'=' * 80}")
    log(f"ğŸš€ å¼€å§‹åŒæ­¥: {BASE_URL} -> {LOCAL_ROOT}")
    log(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"{'=' * 80}\n")
    
    start_time = time.time()
    total_processed, total_skipped = process_directory(BASE_URL, LOCAL_ROOT)
    
    duration = time.time() - start_time
    log(f"\n{'=' * 80}")
    log(f"âœ… åŒæ­¥å®Œæˆ! ç”¨æ—¶: {duration:.2f} ç§’")
    log(f"ğŸ“Š æ€»è®¡: å·²å¤„ç† {total_processed} ä¸ªæ–‡ä»¶ | å·²è·³è¿‡ {total_skipped} ä¸ªæ–‡ä»¶")
    log(f"â° ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    log(f"{'=' * 80}")
    
    # æ·»åŠ ç©ºè¡Œåˆ†éš”æ¯æ¬¡è¿è¡Œ
    with open(LOG_FILE, "a", encoding="utf-8") as log_file:
        log_file.write("\n\n")
